---
title: "Coding Assignment 3"
author: "Team 4: Marko Kuzet, Anthony Morel, Candice Bloom, Melissa Fuentes"
date: "Due: 2021-12-09 23:59"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
#Put any packages you need here

library(plotly)
library(readxl)
library(corrplot)
library(car)


knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 50 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan. 

The data on the 50 customers in the sample is as follows:

-	Charges: Total medical expenses for a particular insurance plan (in dollars)
-	Age: Age of the primary beneficiary
-	BMI: Primary beneficiary’s body mass index (kg/m2)
-	Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-	Children: Number of children covered by health insurance plan (includes other dependents as well)
-	Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-	Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.


```{r dataset, include=FALSE}
# Bring in the dataset here.

insurance <- read_excel("../Data/insurance_0126_Group4.xlsx")

```



## Question 1

Randomly select three observations from the sample and exclude from all modeling (i.e. n=47). Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 47 observations.

```{r q1}

set.seed(123456)
index <- sample(seq_len(nrow(insurance)), size = 3)

train <- insurance[-index,]
test <- insurance[index,]


summary(train)[,c(1:3,5)]

sd(insurance$Charges)
sd(insurance$Age)
sd(insurance$BMI)
sd(insurance$Children)


```


## Question 2

Provide the correlation between all quantitative variables

```{r}
corrplot(cor(train[,c(1:3,5)]), addCoef.col = "red")


```
The corrplot shows that the variables with the highest correlation are Charges and BMI with a value of 0.43, which is a positively weak correlation, higher than the rest of the quantitative variables.



## Question 3

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?

```{r q3 p1}

model_1 <- lm(Charges ~., data = train )
summary(model_1)

par(mfrow=c(2,2))
plot(model_1)
scatterplotMatrix(train[,c(1:3,5)])

```
After running our base model and analyzing the graphs, we clearly see violation of the Gauss-Markov assumptions. For the first graph “Residuals vs. Fitted,” we can see how it does not follow a linear relationship, violating assumption 3.
As for the second graph titled “Normal Q-Q”, there is a violation of assumption 6, due to the graph not being normally distributed. In this graph we can also see a that the pattern
The third plot titled “Scale-Location” checks for homoskwedasticity. If this assumption were not violated, you’d see random points around a horizontal line. In this case, it is upwards sloping, so you can see there is a “fanning out” effect.
The last plot “Residuals vs. Leverage” focuses on the regression outliers, influential observations, and high leverage points, for our data we can see that there are no outliers. 

From here you can see some non-linear relationships and non-normally distributed variables.




## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data  run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r q4 Transformations Theory}
#The following transformations were applied due to the nature of the independent variables: 

#For Charges, we decided to apply log in order to fix the distribution.
train$lnCharges <- log(train$Charges)

#with only transforming Charges we have
scatterplotMatrix(train[,c(10,2,3,5)])

#with only Charges transformation we have these two histograms of a before and after:

hist(train$Charges)
hist(train$lnCharges)

#For both both Age and BMI we can assume the quadratic nature of the variables:
#As far as AGE, the younger we are the more medical attention we would need, if we are analyzing an adult at a mature age they normally would not need much medical attention, and someone who is at an advanced age (elderly) would probably need more medical attention. 


train$AgeSq <- train$Age^2

train$lnAge <- log(train$Age)

#BMI would almost work the same way, the less BMI the average human has the more likely they will need more medical attention (ANOREXIC), if we are observing an average adult at a normal BMI the less medical attention needed, and someone who is at a higher BMI would probably need more medical attention (OBSESITY).


train$BMISq <- train$BMI^2

train$lnBMI <- log(train$BMI)


```



```{r q4 Transformations}
#Applying transformations

#with transforming every variable we get the following scatter plots:

#lnage
scatterplotMatrix(train[,c(10,12,3,5)])

#sqage
#with transforming every variable we get:
scatterplotMatrix(train[,c(10,2,3,5:9,11)])

#lnBMI
scatterplotMatrix(train[,c(10,2,14,5)])

```

```{r q4 Modelsl}
#Creating different models with our transformations:

#Model of lnCharges
model_lncharges <-lm(lnCharges ~., data = train[,c(10,2:9)])
summary(model_lncharges)
plot(model_lncharges)

#Log of Age
model_lnAge <-lm(lnCharges ~., data = train[,c(10,12,3:9)])
summary(model_lnAge)
plot(model_lnAge)

#Age and Age^2
model_Agesquared <-lm(lnCharges ~., data = train[,c(10,2,3:9,11)])
summary(model_Agesquared)
plot(model_Agesquared)

#Log of BMI
model_lnBMI <-lm(lnCharges ~., data = train[,c(10,2,14,4:9)])
summary(model_lnBMI)
plot(model_lnBMI)

#BMI and BMI^2
model_BMIsquared <-lm(lnCharges ~., data = train[,c(10,2,3:9,13)])
summary(model_BMIsquared)
plot(model_BMIsquared)
```
Before the transformation, our fit was 0.7886, so our base model describes 78% of the variance in the dependent variable (charges) that can be explained by the independent variables(Age,BMI,Children..etc.)
<br>
Our first model (lnCharges and everything else the same), we can see that our R^2 is 75%, we have two negative coefficients and 3 significant variables (Age,BMI,Smoker).
<br>
Our second model (lnCharges, lnAge and everything else the same), we can see that our R^2 is 73%, we have two negative coefficients(WinterPark and Oviedo) and 3 significant variables (lnAge,BMI,Smoker).
<br>
Our third transformed model (lnCharges, AgeSq and everything else the same), we can see that our R^2 is 76%, we have two negative coefficients(WinterPark and Oviedo) and 2 significant variables (BMI and Smoker).
<br>
Our fourth transformed model (lnCharges, Age, lnBMI and everything else the same), we can see that our R^2 is 75%, we have two negative coefficients(WinterPark and Oviedo) and 2 significant variables (Age,lnBMI,Smoker).
<br>
Our fifth transformed model (lnCharges, Age, BMISq and everything else the same), we can see that our R^2 is 75%, we have two negative coefficients(WinterPark and Oviedo) and 2 significant variables (Age and Smoker).


## Question 5

Use the 3 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

```{r q5}
test$lncharges <-log(test$Charges)
test$lnAge <- log(test$Age)
test$AgeSq <- test$Age^2
test$lnBMI <- log(test$BMI)
test$BMISq <- test$BMI^2




#bad model
test$model_model_1_pred <- predict(model_1, newdata = test)
#model 1
test$model_lncharges_pred <- predict(model_lncharges, newdata = test) %>% exp()
#model 2
test$model_lnAge_pred <- predict(model_lnAge, newdata = test) %>% exp()
#model 3
test$model_Agesquared_pred <- predict(model_Agesquared, newdata = test) %>% exp()
#model 4
test$model_lnBMI_pred <- predict(model_lnBMI, newdata = test) %>% exp()
#model 5
test$model_BMIsquared_pred <- predict(model_BMIsquared, newdata = test) %>% exp()




#finding the error
test$error_m1 <- test$model_model_1_pred - test$Charges
test$error_1 <- test$model_lncharges_pred - test$Charges
test$error_2 <- test$model_lnAge_pred - test$Charges
test$error_3 <- test$model_Agesquared_pred - test$Charges
test$error_4 <- test$model_lnBMI_pred - test$Charges
test$error_5 <- test$model_BMIsquared_pred - test$Charges


mean(test$error_m1)
mean(test$error_1)
mean(test$error_2)
mean(test$error_3)
mean(test$error_4)
mean(test$error_5)


#MAE
mae <- function(error_vector){
error_vector %>%
abs() %>%
mean()
}


mae(test$error_m1)
mae(test$error_1)
mae(test$error_2)
mae(test$error_3)
mae(test$error_4)
mae(test$error_5)



#RMSE

rmse <- function(error_vector){
error_vector^2 %>%
mean() %>%
sqrt()
}

rmse(test$error_m1)
rmse(test$error_1)
rmse(test$error_2)
rmse(test$error_3)
rmse(test$error_4)
rmse(test$error_5)

#MAPE

mape <- function(error_vector, actual_vector){
(error_vector/actual_vector) %>%
abs() %>%
mean()
}



mape(test$error_m1, test$Charges)
mape(test$error_1, test$Charges)
mape(test$error_2, test$Charges)
mape(test$error_3, test$Charges)
mape(test$error_4, test$Charges)
mape(test$error_5, test$Charges)




```
<br>
After analyzing our performance measures for the 6 models we have noticed that our base model has the lowest bias (-3458.57) and our second to lowest bias is from our second transformed model (lnAge) with a total of 3943.522.
Following up with our second performance measure, MAE, the lowest value is given by our second model (lnAge) with a total of 3943.52, the second lowest would be our third model (AgeSq)with 4042.26.
For our RSME, we noticed that the lowest value was given by our base model 5823.44 followed up by our second model (lnAge) with 5897.224. Being that our values are so close and basing our decision on the other previous measures our group decided to choose the best two models (Base model and lnAge) to continue with the analysis of our data.
<br>

Our group will be looking a decision for short-run analysis. 

<br>



## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.


```{r q6}

summary(model_lnAge)

```

By observing the base regression we can state that 

For every year increase for Age, the price increases by $0.97077
For every unit increase for BMI, the price increases by $0.04010. 
If you are a Female the price increases by $0.14205.
If you have Children the price increases by $0.02152.
If the individual is a smoker, the price increases by $1.33688. 
If you live in WinterSprings it's $0.27634 more expensive than if you live in 
Sanford.
If you live in WinterPark the price is $0.07151 cheaper than if you live in Sanford.
If you live in Oviedo the price goes down by $0.15994 than if you live in Sanford.

Age, BMI, and Smoker tested significant in the initial model. The rest did not test significant. 

$$Charges = 4.11544+0.97077*lnAge+0.04010*BMI+0.02152*Children + 0.14205 *Female + 1.33688*Smoker + 0.27634 *WinterSprings-0.07151*WinterPark-0.15994*Oviedo$$

## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
| -------- | --- | --- | ------ | -------- | ------ | -------------- | 
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |

```{r q7}
#Based on our findings, the better of the two models mentioned on question #5, our group decided 


customer1 <- data.frame(lnAge=60,BMI=22,Female=1,Children=0,Smoker=0, WinterSprings = 0, WinterPark = 0, Oviedo = 1)
predict(model_lnAge, newdata = customer1,interval = "prediction" )

customer2 <- data.frame(lnAge=40,BMI=30,Female=0,Children=2,Smoker=0,WinterSprings = 0, WinterPark = 0, Oviedo = 0)
predict(model_lnAge, newdata = customer2,interval = "prediction" )

customer3 <- data.frame(lnAge=25,BMI=25,Female=0,Children=0,Smoker=1,WinterSprings = 0, WinterPark = 1, Oviedo = 0)
predict(model_lnAge, newdata = customer3,interval = "prediction" )

customer4 <- data.frame(lnAge=33,BMI=35,Female=1,Children=0,Smoker=0, WinterSprings = 1, WinterPark = 0, Oviedo = 0)
predict(model_lnAge, newdata = customer4,interval = "prediction" )

customer5 <- data.frame(lnAge=45,BMI=27,Female=1,Children=3,Smoker=0, WinterSprings = 0, WinterPark = 0, Oviedo = 1)
predict(model_lnAge, newdata = customer5,interval = "prediction" )
 



```

By using our LnAge Model we are able to find the prediction intervals for the five potential clients.
For customer 1 the price will be $63 give or take $21 less or more. 
For Customer 2 the price will be $44 give or take $13. 
For Customer 3 the price will be $30 give or take $8. 
For Customer 4 the price will be $38 give or take $11. 
For Customer 5 the price will be $49 give or take $15.




## Question 8

The owner notices that some of the predictions are wider than others, explain why.

Since we are considering prediction intervals they must account for both the uncertainty in estimating the population mean and the random variation of individual values. Prediction intervals are always wider than the confidence interval due to their nature of predicting ranges for individual (specific) observations rather than the mean value. Also the prediction interval will not converge to a single value as the sample size increases.

The owner notices this because the prediction interval is always wider than the confidence interval of the prediction because of the added uncertainty involved in predicting a single response versus the mean response. We can see that with customers 1 and 5 have the highest age along with the widest range. From the customers used in the model the higher the age the higher the range in is effect in the interval.


https://www.graphpad.com/support/faq/the-distinction-between-confidence-intervals-prediction-intervals-and-tolerance-intervals/


## Question 9 

Are there any prediction problems that occur with the five potential clients? If so, explain.
```{r q9}
summary(train[,c(1:3,5)])

```
We notice how all the customers fall withing the ranges of each independent variables, there is no extrapolation here since there are no dependent value outside the range of the independent data.




